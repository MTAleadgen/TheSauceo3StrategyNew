# TheSauceo3PlanNew Event Scraper

This project implements a data pipeline to scrape event information using SerpApi, enrich it with Google Places, clean descriptions using an AI model (Qwen placeholder), and store it in a Supabase database.

## Project Structure

```
.
├── data/
│   ├── admin1CodesASCII.txt      # (Downloaded) GeoNames admin level 1 codes
│   ├── cities15000.txt         # (Downloaded and deleted after use) GeoNames cities with >15000 population
│   ├── cities_shortlist.csv      # Generated shortlist of cities in the Americas for processing
│   └── generate_shortlist.py     # Script to generate cities_shortlist.csv
├── pipelines/
│   └── serpapi/
│       └── events/
│           ├── parser.py             # Parses raw SerpApi event results
│           ├── places_enricher.py    # Enriches event data with Google Places API
│           ├── qwen_cleaner.py       # Cleans event descriptions (placeholder AI call)
│           └── request_builder.py    # Builds SerpApi request parameters
├── runner/
│   └── cli.py                    # Main CLI script to orchestrate the pipeline
├── .env.sample                   # Sample environment file (copy to .env and fill in)
├── .gitignore                    # Specifies intentionally untracked files
├── README.md                     # This file
└── requirements.txt              # Python dependencies
```

## Setup

1.  **Clone the repository.**
2.  **Create a Python virtual environment and activate it.**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # On Windows: .venv\Scripts\activate
    ```
3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
4.  **Set up environment variables:**
    *   Copy `.env.sample` to a new file named `.env`.
    *   Fill in your API keys and Supabase credentials in the `.env` file:
        *   `SERPAPI_API_KEY`: Your SerpApi key.
        *   `GOOGLE_PLACES_API_KEY`: Your Google Cloud Places API key (ensure Places API is enabled).
        *   `LAMBDA_API_KEY`: API key for your Qwen model endpoint (if using a custom endpoint).
        *   `SUPABASE_URL`: Your Supabase project URL.
        *   `SUPABASE_KEY`: Your Supabase `service_role` key (found in Project Settings > API).
        *   `SMTP_HOST`, `SMTP_PORT`, `SMTP_USER`, `SMTP_PASS`, `EMAIL_FROM`, `EMAIL_TO`: (Optional) For email summaries. If `SMTP_HOST` is not set, the summary will be logged to console.

5.  **(Optional) Generate city shortlist:**
    The `data/cities_shortlist.csv` is included, but you can regenerate it if needed (e.g., to update city data from GeoNames):
    ```bash
    python data/generate_shortlist.py
    ```
    This script downloads city and admin code data from GeoNames, filters for cities in the Americas, sorts by population, and saves the top 1250 to `data/cities_shortlist.csv`.

## Running the Pipeline

The main entry point is `runner/cli.py`.

```bash
python -m runner.cli --mode serpapi_events --cities data/cities_shortlist.csv [OPTIONS]
```

**Required Arguments:**
*   `--mode serpapi_events`: Specifies the pipeline to run (currently only SerpApi events).
*   `--cities <PATH_TO_CSV>`: Path to the CSV file containing city data (e.g., `data/cities_shortlist.csv`). This file must include `name`, `latitude`, `longitude`, `hl`, and `gl` columns.

**Options:**
*   `--max-cities <N>`: Limit processing to the first N cities from the CSV file.
*   `--max-events <N>`: Maximum number of events to try and fetch per city. Default: `100`.
*   `--days-forward <N>`: How many days into the future to include events for (from today). Default: `30`.
*   `--batch-size <N>`: (Currently informational) Number of cities intended per conceptual batch. The script processes city by city. Default: `50`.

**SerpApi Event Fetching Details:**
*   **Location Parameter (`uule`):** Event searches use the `uule` parameter, generated from city latitude and longitude using the `pyuule` library. This provides a Google-canonical location encoding.
*   **Pagination:** The script fetches events in pages (20 events per SerpApi call) up to the `--max-events` limit or until SerpApi returns no more results for a city.
*   **Date Filtering:** Events are filtered to include only those starting within the next `--days-forward` days from the current date.
*   **Source ID:** A deterministic `source_id` for each event is generated by creating an MD5 hash of the event's `link`.
*   **Retries:** SerpApi calls have built-in exponential backoff and retry (up to 3 times) for transient network errors (like `ConnectionResetError`).

**Example Smoke Test (fetch up to 40 events within next 15 days for NYC):**
```bash
python -m runner.cli --mode serpapi_events --cities data/cities_shortlist.csv --max-cities 1 --max-events 40 --days-forward 15
```
(Ensure NYC is the first city in your `cities_shortlist.csv` or adjust `--max-cities` and point to a test CSV for this specific command).

## Pipeline Steps (for `serpapi_events` mode)

1.  **Load Cities:** Reads cities from the specified CSV.
2.  **For each city:**
    a.  **Build Base Parameters:** Creates base SerpApi request parameters, including the `uule` location string.
    b.  **Fetch SerpApi Events (Paginated):**
        *   Calls SerpApi `google_events` engine page by page (`num=20` results per page).
        *   Continues until `--max-events` are collected, or no more events are found for that city, or an unrecoverable API error occurs.
    c.  **Process Events:** For each raw event found:
        i.  **Parse & Filter:** Maps raw data to a structured format. Generates `source_id` from `link`. Filters out events starting beyond `--days-forward`.
        ii. **Enrich (Google Places):** Uses `places_enricher.py` to fill in missing address details or lat/lng using Google Places API (Text Search + Details, with in-memory caching).
        iii. **Clean (Qwen AI):** Uses `qwen_cleaner.py` to rewrite the event description (placeholder for actual AI model call).
        iv. **Upsert to Supabase:** Saves the processed event data to the `events` table in your Supabase database. Uses `event_day`, `venue`, and `name` as conflict resolution keys.
3.  **Summarize Run:** Prints a JSON summary of the run (or sends an email if SMTP is configured).

## Development Notes
*   The `qwen_cleaner.py` currently has a placeholder for the AI model API call. You will need to replace `call_ai_model` with your actual implementation.
*   Error handling is included, but monitor logs for any persistent issues.
